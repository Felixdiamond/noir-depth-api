"""
DepthAnything V2 Service for Project Noir
Provides monocular depth estimation using official DepthAnything V2 PyTorch implementation
Optimized for NVIDIA L4 GPUs on Google Cloud Run
"""


# Set environment for headless operation before any imports
import os
os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '1'

# Ensure Depth Anything V2 repo is importable
import sys
sys.path.append('/app/Depth-Anything-V2')

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import io
import logging
import time
import base64
from typing import Optional, Dict, Any
from pydantic import BaseModel
from contextlib import asynccontextmanager
import asyncio

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Response models
class DepthResult(BaseModel):
    depth_map_base64: str
    colorized_depth_base64: str
    min_depth: float
    max_depth: float
    avg_depth: float
    processing_time: float
    image_width: int
    image_height: int
    model_used: str

class DepthAnalysis(BaseModel):
    closest_distance: float
    farthest_distance: float
    center_depth: float
    depth_distribution: Dict[str, float]  # near, medium, far percentages

# Global model variable
depth_model = None
device = None

# DepthAnything V2 Model Configuration
model_configs = {
    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
}


# We'll use the small model (vits) for efficiency
MODEL_TYPE = 'vits'  # 24.8M parameters - good balance of speed and accuracy
MODEL_PATH = '/app/checkpoints/depth_anything_v2_vits.pth'




def load_depthanything_model():
    """Load DepthAnything V2 model (official only, fail fast if missing)"""
    global depth_model, device
    try:
        # Determine device
        if torch.cuda.is_available():
            device = torch.device('cuda')
            logger.info(f"üöÄ Using CUDA GPU: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device('cpu')
            logger.warning("‚ö†Ô∏è CUDA not available, using CPU")

        # Import official DepthAnything V2 implementation
        try:
            from depth_anything_v2.dpt import DepthAnythingV2 as OfficialDepthAnythingV2
        except ImportError as e:
            logger.error("‚ùå Official DepthAnything V2 not found in /app/Depth-Anything-V2. Check Dockerfile and PYTHONPATH.")
            raise

        config = model_configs[MODEL_TYPE]
        depth_model = OfficialDepthAnythingV2(**config)

        # Load pretrained weights
        if os.path.exists(MODEL_PATH):
            logger.info(f"üì¶ Loading pretrained weights from {MODEL_PATH}")
            checkpoint = torch.load(MODEL_PATH, map_location='cpu')
            depth_model.load_state_dict(checkpoint)
        else:
            logger.error(f"‚ùå Pretrained weights not found at {MODEL_PATH}. Check Dockerfile download step.")
            raise FileNotFoundError(f"Model weights not found at {MODEL_PATH}")

        # Move to device and set eval mode
        depth_model = depth_model.to(device).eval()
        logger.info(f"‚úÖ DepthAnything V2 {MODEL_TYPE.upper()} model loaded successfully")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to load DepthAnything V2 model: {e}")
        return False

def analyze_depth_map(depth_map: np.ndarray) -> DepthAnalysis:
    """Analyze depth map to extract useful metrics"""
    
    # Basic statistics
    min_depth = float(np.min(depth_map))
    max_depth = float(np.max(depth_map))
    
    # Center depth (useful for navigation)
    h, w = depth_map.shape
    center_h, center_w = h // 2, w // 2
    center_region = depth_map[center_h-20:center_h+20, center_w-20:center_w+20]
    center_depth = float(np.mean(center_region))
    
    # Depth distribution
    total_pixels = depth_map.size
    near_threshold = min_depth + (max_depth - min_depth) * 0.3
    far_threshold = min_depth + (max_depth - min_depth) * 0.7
    
    near_pixels = np.sum(depth_map < near_threshold)
    medium_pixels = np.sum((depth_map >= near_threshold) & (depth_map < far_threshold))
    far_pixels = np.sum(depth_map >= far_threshold)
    
    depth_distribution = {
        "near": round(near_pixels / total_pixels * 100, 1),
        "medium": round(medium_pixels / total_pixels * 100, 1),
        "far": round(far_pixels / total_pixels * 100, 1)
    }
    
    return DepthAnalysis(
        closest_distance=min_depth,
        farthest_distance=max_depth,
        center_depth=center_depth,
        depth_distribution=depth_distribution
    )

def create_colorized_depth(depth_map: np.ndarray) -> np.ndarray:
    """Create colorized depth map for visualization"""
    # Normalize depth to 0-255
    depth_normalized = ((depth_map - depth_map.min()) / (depth_map.max() - depth_map.min()) * 255).astype(np.uint8)
    
    # Apply colormap (plasma is good for depth)
    colorized = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_PLASMA)
    
    return colorized

def numpy_to_base64(img_array: np.ndarray) -> str:
    """Convert numpy array to base64 string"""
    _, buffer = cv2.imencode('.png', img_array)
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle"""
    logger.info("üöÄ Starting DepthAnything V2 Service")
    
    # Load model on startup
    success = await asyncio.to_thread(load_depthanything_model)
    if not success:
        logger.error("‚ùå Failed to initialize depth model")
        raise RuntimeError("Model initialization failed")
    
    logger.info("‚úÖ DepthAnything V2 Service ready")
    yield
    
    logger.info("üõë Shutting down DepthAnything V2 Service")

class DepthEstimationService:
    """Service wrapper for DepthAnything V2 model"""
    
    def __init__(self):
        """Initialize the service and load the model"""
        global depth_model, device
        
        success = load_depthanything_model()
        if not success:
            raise RuntimeError("Failed to initialize depth model")
        
        self.model = depth_model
        self.device = device
    
    def estimate_depth(self, image: np.ndarray) -> np.ndarray:
        """Estimate depth from image"""
        return self.model.infer_image(image)
    
    def analyze_depth(self, depth_map: np.ndarray) -> DepthAnalysis:
        """Analyze depth map"""
        return analyze_depth_map(depth_map)

# Create FastAPI app
app = FastAPI(
    title="Project Noir - DepthAnything V2 Service",
    description="Monocular depth estimation using DepthAnything V2 for assistive navigation",
    version="2.0.0",
    lifespan=lifespan
)

@app.get("/")
async def root():
    return {
        "service": "Project Noir DepthAnything V2",
        "version": "2.0.0",
        "model": f"DepthAnything V2 {MODEL_TYPE.upper()}",
        "device": str(device) if device else "not_initialized",
        "status": "ready" if depth_model else "initializing"
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy" if depth_model else "unhealthy",
        "model_loaded": depth_model is not None,
        "device": str(device) if device else "unknown",
        "gpu_available": torch.cuda.is_available(),
        "model_type": MODEL_TYPE
    }

@app.post("/estimate-depth", response_model=DepthResult)
async def estimate_depth(
    file: UploadFile = File(...),
    return_analysis: bool = False,
    return_colorized: bool = True
):
    """
    Estimate depth from uploaded image
    """
    if not depth_model:
        raise HTTPException(status_code=503, detail="Depth model not loaded")
    
    try:
        start_time = time.time()
        
        # Read and decode image
        image_bytes = await file.read()
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise HTTPException(status_code=400, detail="Invalid image format")
        
        h, w = image.shape[:2]
        logger.info(f"üñºÔ∏è Processing image: {w}x{h}")
        
        # Run depth estimation
        depth_map = await asyncio.to_thread(depth_model.infer_image, image)
        
        # Basic statistics
        min_depth = float(np.min(depth_map))
        max_depth = float(np.max(depth_map))
        avg_depth = float(np.mean(depth_map))
        
        # Convert depth map to base64
        depth_normalized = ((depth_map - min_depth) / (max_depth - min_depth) * 255).astype(np.uint8)
        depth_base64 = numpy_to_base64(depth_normalized)
        
        # Create colorized depth map
        colorized_depth = create_colorized_depth(depth_map)
        colorized_base64 = numpy_to_base64(colorized_depth) if return_colorized else ""
        
        processing_time = time.time() - start_time
        
        logger.info(f"‚úÖ Depth estimation completed in {processing_time:.3f}s")
        logger.info(f"üìä Depth range: {min_depth:.3f} - {max_depth:.3f} (avg: {avg_depth:.3f})")
        
        return DepthResult(
            depth_map_base64=depth_base64,
            colorized_depth_base64=colorized_base64,
            min_depth=min_depth,
            max_depth=max_depth,
            avg_depth=avg_depth,
            processing_time=processing_time,
            image_width=w,
            image_height=h,
            model_used=f"DepthAnything V2 {MODEL_TYPE}"
        )
        
    except Exception as e:
        logger.error(f"‚ùå Depth estimation error: {e}")
        raise HTTPException(status_code=500, detail=f"Depth estimation failed: {str(e)}")

@app.post("/analyze-depth", response_model=Dict[str, Any])
async def analyze_depth_endpoint(file: UploadFile = File(...)):
    """
    Analyze depth for navigation assistance
    """
    if not depth_model:
        raise HTTPException(status_code=503, detail="Depth model not loaded")
    
    try:
        start_time = time.time()
        
        # Read and process image
        image_bytes = await file.read()
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise HTTPException(status_code=400, detail="Invalid image format")
        
        # Run depth estimation
        depth_map = await asyncio.to_thread(depth_model.infer_image, image)
        
        # Analyze depth map
        analysis = analyze_depth_map(depth_map)
        
        processing_time = time.time() - start_time
        
        return {
            "analysis": analysis.dict(),
            "processing_time": processing_time,
            "navigation_guidance": {
                "obstacle_warning": analysis.center_depth < 0.3,
                "safe_path": analysis.center_depth > 0.5,
                "depth_quality": "good" if analysis.farthest_distance > 0.8 else "limited"
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Depth analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"Depth analysis failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
